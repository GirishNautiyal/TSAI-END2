{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "END2_Assignment_5.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYiRsFGD6iUC"
      },
      "source": [
        "# 0 TorchText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IwvZKE_3sZH8"
      },
      "source": [
        "Stanford Sentiment Treebank V1.0\n",
        "\n",
        "This is the dataset of the paper:\n",
        "\n",
        "Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank\n",
        "Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng and Christopher Potts\n",
        "Conference on Empirical Methods in Natural Language Processing (EMNLP 2013)\n",
        "\n",
        "If you use this dataset in your research, please cite the above paper.\n",
        "\n",
        "@incollection{SocherEtAl2013:RNTN,\n",
        "title = {{Parsing With Compositional Vector Grammars}},\n",
        "author = {Richard Socher and Alex Perelygin and Jean Wu and Jason Chuang and Christopher Manning and Andrew Ng and Christopher Potts},\n",
        "booktitle = {{EMNLP}},\n",
        "year = {2013}\n",
        "}\n",
        "\n",
        "This file includes:\n",
        "1. original_rt_snippets.txt contains 10,605 processed snippets from the original pool of Rotten Tomatoes HTML files. Please note that some snippet may contain multiple sentences.\n",
        "\n",
        "2. dictionary.txt contains all phrases and their IDs, separated by a vertical line |\n",
        "\n",
        "3. sentiment_labels.txt contains all phrase ids and the corresponding sentiment labels, separated by a vertical line.\n",
        "Note that you can recover the 5 classes by mapping the positivity probability using the following cut-offs:\n",
        "[0, 0.2], (0.2, 0.4], (0.4, 0.6], (0.6, 0.8], (0.8, 1.0]\n",
        "for very negative, negative, neutral, positive, very positive, respectively.\n",
        "Please note that phrase ids and sentence ids are not the same.\n",
        "\n",
        "4. SOStr.txt and STree.txt encode the structure of the parse trees. \n",
        "STree encodes the trees in a parent pointer format. Each line corresponds to each sentence in the datasetSentences.txt file. The Matlab code of this paper will show you how to read this format if you are not familiar with it.\n",
        "\n",
        "5. datasetSentences.txt contains the sentence index, followed by the sentence string separated by a tab. These are the sentences of the train/dev/test sets.\n",
        "\n",
        "6. datasetSplit.txt contains the sentence index (corresponding to the index in datasetSentences.txt file) followed by the set label separated by a comma:\n",
        "\t1 = train\n",
        "\t2 = test\n",
        "\t3 = dev\n",
        "\n",
        "Please note that the datasetSentences.txt file has more sentences/lines than the original_rt_snippet.txt. \n",
        "Each row in the latter represents a snippet as shown on RT, whereas the former is each sub sentence as determined by the Stanford parser.\n",
        "\n",
        "For comparing research and training models, please use the provided train/dev/test splits.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp5IzBGsPGHs"
      },
      "source": [
        "## Dataset Preview\n",
        "\n",
        "Your first step to deep learning in NLP. We will be mostly using PyTorch. Just like torchvision, PyTorch provides an official library, torchtext, for handling text-processing pipelines. \n",
        "\n",
        "We will be using previous session tweet dataset. Let's just preview the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1-Yz-5RRFYc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "40268e4a-9aad-41a5-b890-9ab35b44aae7"
      },
      "source": [
        "import pandas as pd\n",
        "df1 = pd.read_table('content/datasetSentences.txt')\n",
        "df1.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_index</th>\n",
              "      <th>sentence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sentence_index                                           sentence\n",
              "0               1  The Rock is destined to be the 21st Century 's...\n",
              "1               2  The gorgeously elaborate continuation of `` Th...\n",
              "2               3                     Effective but too-tepid biopic\n",
              "3               4  If you sometimes like to go to the movies to h...\n",
              "4               5  Emerges as something rare , an issue movie tha..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3QudvDmas5q"
      },
      "source": [
        "split = pd.read_csv('content/datasetSplit.txt', sep=\",\").set_index('sentence_index')\n",
        "sentence = pd.read_csv('content/datasetSentences.txt', sep=\"\\t\").set_index('sentence_index')\n",
        "\n",
        "#df = pd.concat([sentence_idx,sentence, lab],axis=1 ,sort=False)\n",
        "df = pd.concat([sentence, split],axis=1 ,sort=False)\n",
        "\n",
        "df.to_csv('content/Combined_sentiment.csv', index=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R7JdpCW-YbAG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcb113ba-a7b3-4402-b795-a9c41d1c0217"
      },
      "source": [
        "df.shape\n",
        "#1364 tweets, 2 columns, tweet and label"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11855, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "id": "-lMmDgNDABOo",
        "outputId": "78c39182-5ff8-4b07-8e90-32e6669a348a"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>sentence_index</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>The Rock is destined to be the 21st Century 's...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The gorgeously elaborate continuation of `` Th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Effective but too-tepid biopic</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>If you sometimes like to go to the movies to h...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Emerges as something rare , an issue movie tha...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                         sentence  splitset_label\n",
              "sentence_index                                                                   \n",
              "1               The Rock is destined to be the 21st Century 's...               1\n",
              "2               The gorgeously elaborate continuation of `` Th...               1\n",
              "3                                  Effective but too-tepid biopic               2\n",
              "4               If you sometimes like to go to the movies to h...               2\n",
              "5               Emerges as something rare , an issue movie tha...               2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gf7UswGhhibZ",
        "outputId": "7f50eb6c-24ec-4249-bee0-d777764aadbe"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['sentence', 'splitset_label'], dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2qWANwPPMz0"
      },
      "source": [
        "sentiment_label = pd.read_csv('content/sentiment_labels.txt', sep=\"|\").set_index('phrase ids', 'sentiment values')\n",
        "dictionary = pd.read_csv('content/dictionary.txt', sep=\"|\").set_index('!')\n",
        "sentiment_label.columns\n",
        "dictionary.columns\n",
        "df3 = pd.concat([sentiment_label, dictionary],axis=1 ,sort=False)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        },
        "id": "3iYFJxSLTIZ8",
        "outputId": "d9b51a3d-75e9-40ee-e183-7e8420428b08"
      },
      "source": [
        "df3"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentiment values</th>\n",
              "      <th>0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.50000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.50000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.44444</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.50000</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.42708</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The New Guy</th>\n",
              "      <td>NaN</td>\n",
              "      <td>19286.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The New Guy does have a heart .</th>\n",
              "      <td>NaN</td>\n",
              "      <td>226144.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The New Guy does have a heart . Now , if it only had a brain .</th>\n",
              "      <td>NaN</td>\n",
              "      <td>19287.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The New Guy is one of them .</th>\n",
              "      <td>NaN</td>\n",
              "      <td>149126.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>The Next Great Thing</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>289558 rows × 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                    sentiment values         0\n",
              "0                                                            0.50000       NaN\n",
              "1                                                            0.50000       NaN\n",
              "2                                                            0.44444       NaN\n",
              "3                                                            0.50000       NaN\n",
              "4                                                            0.42708       NaN\n",
              "...                                                              ...       ...\n",
              "The New Guy                                                      NaN   19286.0\n",
              "The New Guy does have a heart .                                  NaN  226144.0\n",
              "The New Guy does have a heart . Now , if it onl...               NaN   19287.0\n",
              "The New Guy is one of them .                                     NaN  149126.0\n",
              "The Next Great Thing                                             NaN       NaN\n",
              "\n",
              "[289558 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TXx-9AhbGlv"
      },
      "source": [
        "df = pd.read_csv('content/Combined_sentiment.csv')\n",
        "\n",
        "df['sentence'] = df.sentence.str.replace(\"'s\", '') # Removes nonalphabetic\n",
        "df['sentence'] = df.sentence.str.lower()\n",
        "\n",
        "df['sentence'] = df.sentence.str.replace('[^a-zA-Z ]', '') # Removes nonalphabetic\n",
        "df['sentence'] = df.sentence.str.replace('  ', '') # Removes double-space"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "TtIiqhEjrCV2",
        "outputId": "cffc17f6-e27b-4023-926a-59c54a53003e"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>splitset_label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>the rock is destined to be the st centurynewco...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>the gorgeously elaborate continuation ofthe lo...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>effective but tootepid biopic</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>if you sometimes like to go to the movies to h...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>emerges as something rarean issue movie thatso...</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            sentence  splitset_label\n",
              "0  the rock is destined to be the st centurynewco...               1\n",
              "1  the gorgeously elaborate continuation ofthe lo...               1\n",
              "2                      effective but tootepid biopic               2\n",
              "3  if you sometimes like to go to the movies to h...               2\n",
              "4  emerges as something rarean issue movie thatso...               2"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zqRsoF6xYdgl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa7da57-5e2f-41a5-eff2-5bb3ad1ed176"
      },
      "source": [
        "df.splitset_label.value_counts()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    8544\n",
              "2    2210\n",
              "3    1101\n",
              "Name: splitset_label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJ6o_79ISSVb"
      },
      "source": [
        "## Defining Fields"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e63g08ijOrf7"
      },
      "source": [
        "Now we shall be defining LABEL as a LabelField, which is a subclass of Field that sets sequential to False (as it’s our numerical category class). TWEET is a standard Field object, where we have decided to use the spaCy tokenizer and convert all the text to lower‐ case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk8IP4SK1Lrp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4656a41-aab2-4c95-9827-3fa7a96a6fb3"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f561108c170>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJeN6VLt3nS-"
      },
      "source": [
        "Field class models common text processing datatypes that can be represented by tensors.  It holds a Vocab object that defines the set of possible values for elements of the field and their corresponding numerical representations.\n",
        "The Field object also holds other parameters relating to how a datatype should be numericalized, such as a tokenization method and the kind of Tensor that should be produced.\n",
        "\n",
        "Attributes:\n",
        "sequential: Whether the datatype represents sequential data. If False, no tokenization is applied. Default: True.\n",
        "use_vocab: Whether to use a Vocab object. If False, the data in this field should already be numerical. Default: True.\n",
        "init_token: A token that will be prepended to every example using this field, or None for no initial token. Default: None.\n",
        "fix_length: A fixed length that all examples using this field will bepadded to, or None for flexible sequence lengths. Default: None.\n",
        "dtype: The torch.dtype class that represents a batch of examples of this kind of data. Default: torch.long.\n",
        "preprocessing: The Pipeline that will be applied to examples using this field after tokenizing but before numericalizing. Many Datasets replace this attribute with a custom preprocessor.Default: None.\n",
        "postprocessing: A Pipeline that will be applied to examples using this field after numericalizing but before the numbers are turned\n",
        "into a Tensor. The pipeline function takes the batch as a list, and the field's Vocab. Default: None.\n",
        "lower: Whether to lowercase the text in this field. Default: False.\n",
        "tokenize: The function used to tokenize strings using this field into sequential examples. If \"spacy\", the SpaCy tokenizer is used. If a non-serializable function is passed as an argument,the field will not be able to be serialized. Default: string.split.\n",
        "tokenizer_language: The language of the tokenizer to be constructed. Various languages currently supported only in SpaCy.\n",
        "include_lengths: Whether to return a tuple of a padded minibatch and a list containing the lengths of each examples, or just a padded minibatch. Default: False.\n",
        "batch_first: Whether to produce tensors with the batch dimension first.Default: False.\n",
        "pad_token: The string token used as padding. Default: \"<pad>\".\n",
        "unk_token: The string token used to represent OOV words. Default: \"<unk>\".\n",
        "pad_first: Do the padding of the sequence at the beginning. Default: False.\n",
        "truncate_first: Do the truncating of the sequence at the beginning. Default: False\n",
        "stop_words: Tokens to discard during the preprocessing step. Default: None\n",
        "is_target: Whether this field is a target variable. Affects iteration over batches. Default: False\n",
        "\n",
        "A label field is a shallow wrapper around a standard field designed to hold labels for a classification task. Its only use is to set the unk_token and sequential to `None` by default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l7IVgHwqfG6q",
        "outputId": "779226b6-f333-4a93-b50a-5f23a6d274fe"
      },
      "source": [
        "# Import Library\n",
        "import random\n",
        "import torch, torchtext\n",
        "from torchtext.legacy import data \n",
        "\n",
        "# Manual Seed\n",
        "SEED = 43\n",
        "torch.manual_seed(SEED)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f561108c170>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u6bKQax2Mf_U"
      },
      "source": [
        "sentence = data.Field(sequential = True, tokenize = 'spacy', batch_first =True, include_lengths=True)\n",
        "split = data.LabelField(tokenize ='spacy', is_target=True, batch_first =True, sequential =False)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX-lYIe_O7Vy"
      },
      "source": [
        "Having defined those fields, we now need to produce a list that maps them onto the list of rows that are in the CSV:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VawdWq36O6td"
      },
      "source": [
        "fields = [('tweets', sentence),('labels',split)]"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbtZ-Ph2P1xL"
      },
      "source": [
        "Armed with our declared fields, lets convert from pandas to list to torchtext. We could also use TabularDataset to apply that definition to the CSV directly but showing an alternative approach too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3OLcJ5B7rHz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "149dd754-ea5c-4848-d678-d4d8166b157f"
      },
      "source": [
        "example = [data.Example.fromlist([df.sentence[i],df.splitset_label[i]], fields) for i in range(df.shape[0])] \n",
        "vars(example[10])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 2,\n",
              " 'tweets': ['take',\n",
              "  'care',\n",
              "  'of',\n",
              "  'my',\n",
              "  'cat',\n",
              "  'offers',\n",
              "  'a',\n",
              "  'refreshingly',\n",
              "  'different',\n",
              "  'slice',\n",
              "  'of',\n",
              "  'asian',\n",
              "  'cinema']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nT-flpH-P1cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a62ac04-2ec1-461c-cfef-19be01844533"
      },
      "source": [
        "stanfordDataset = data.Dataset(example, fields)\n",
        "vars(stanfordDataset[10])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 2,\n",
              " 'tweets': ['take',\n",
              "  'care',\n",
              "  'of',\n",
              "  'my',\n",
              "  'cat',\n",
              "  'offers',\n",
              "  'a',\n",
              "  'refreshingly',\n",
              "  'different',\n",
              "  'slice',\n",
              "  'of',\n",
              "  'asian',\n",
              "  'cinema']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6ZnyCPaR08F"
      },
      "source": [
        "Finally, we can split into training, testing, and validation sets by using the split() method:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPYXyuKhRpBk"
      },
      "source": [
        "(train, valid) = stanfordDataset.split(split_ratio=[0.85, 0.15], random_state=random.seed(SEED))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ykvsCGQMR6UD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a774dc6-b1d9-480c-dd11-99e9dec540c3"
      },
      "source": [
        "(len(train), len(valid))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10077, 1778)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kix8P2IKSBaV"
      },
      "source": [
        "An example from the dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_B3w5Kf60jR",
        "outputId": "58848ca9-125b-44b7-e6cd-16c8006009de"
      },
      "source": [
        "vars(valid.examples[10])"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 1, 'tweets': ['in', 'other', 'wordsitbadder', 'than', 'bad']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUpEOQruR9JL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d5dde6c-3654-44a2-eee0-aa01c2440eaa"
      },
      "source": [
        "vars(train.examples[10])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'labels': 1,\n",
              " 'tweets': ['in',\n",
              "  'other',\n",
              "  'wordsabout',\n",
              "  'as',\n",
              "  'bad',\n",
              "  'a',\n",
              "  'film',\n",
              "  'you',\n",
              "  're',\n",
              "  'likely',\n",
              "  'to',\n",
              "  'see',\n",
              "  'all',\n",
              "  'year']}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKdllP3FST4N"
      },
      "source": [
        "## Building Vocabulary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuvWQ-SpSmSz"
      },
      "source": [
        "At this point we would have built a one-hot encoding of each word that is present in the dataset—a rather tedious process. Thankfully, torchtext will do this for us, and will also allow a max_size parameter to be passed in to limit the vocabu‐ lary to the most common words. This is normally done to prevent the construction of a huge, memory-hungry model. We don’t want our GPUs too overwhelmed, after all. \n",
        "\n",
        "Let’s limit the vocabulary to a maximum of 5000 words in our training set:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx955u93SGeY"
      },
      "source": [
        "sentence.build_vocab(train)\n",
        "split.build_vocab(train)"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xvyEeEjXTGhX"
      },
      "source": [
        "By default, torchtext will add two more special tokens, <unk> for unknown words and <pad>, a padding token that will be used to pad all our text to roughly the same size to help with efficient batching on the GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rA3tIESdcJdN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc714bec-95e1-4c77-a374-6a33ac92a553"
      },
      "source": [
        "print('Size of input vocab : ', len(sentence.vocab))\n",
        "print('Size of label vocab : ', len(split.vocab))\n",
        "print('Top 20 words appreared repeatedly :', list(sentence.vocab.freqs.most_common(20)))\n",
        "print('Labels : ', split.vocab.stoi)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of input vocab :  26545\n",
            "Size of label vocab :  3\n",
            "Top 20 words appreared repeatedly : [('the', 7944), ('a', 5730), ('of', 5043), ('and', 4266), ('to', 3524), ('is', 2762), ('in', 2089), ('that', 1954), ('it', 1662), ('as', 1420), ('with', 1170), ('for', 1162), ('its', 1065), ('this', 1057), ('an', 1051), ('film', 1035), ('movie', 937), ('you', 856), ('nt', 784), ('be', 779)]\n",
            "Labels :  defaultdict(None, {1: 0, 2: 1, 3: 2})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwjD2-ebTeUX"
      },
      "source": [
        "**Lots of stopwords!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLWW221gTpNs"
      },
      "source": [
        "Now we need to create a data loader to feed into our training loop. Torchtext provides the BucketIterator method that will produce what it calls a Batch, which is almost, but not quite, like the data loader we used on images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQqMhMoDUDmn"
      },
      "source": [
        "But at first declare the device we are using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zfo2QhGJUK4l"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zK2ORoqdTNsM"
      },
      "source": [
        "train_iterator, valid_iterator = data.BucketIterator.splits((train, valid), batch_size = 32, \n",
        "                                                            sort_key = lambda x: len(x.tweets),\n",
        "                                                            sort_within_batch=True, device = device)"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gg7gTFQO4fby"
      },
      "source": [
        "Save the vocabulary for later use"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niE9Cc6-2bD_"
      },
      "source": [
        "import os, pickle\n",
        "with open('tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(sentence.vocab.stoi, tokens)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1AbsQwqkVyAy"
      },
      "source": [
        "## Defining Our Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4PED4HJWH4t"
      },
      "source": [
        "We use the Embedding and LSTM modules in PyTorch to build a simple model for classifying tweets.\n",
        "\n",
        "In this model we create three layers. \n",
        "1. First, the words in our tweets are pushed into an Embedding layer, which we have established as a 300-dimensional vector embedding. \n",
        "2. That’s then fed into a 2 stacked-LSTMs with 100 hidden features (again, we’re compressing down from the 300-dimensional input like we did with images). We are using 2 LSTMs for using the dropout.\n",
        "3. Finally, the output of the LSTM (the final hidden state after processing the incoming tweet) is pushed through a standard fully connected layer with three outputs to correspond to our three possible classes (negative, positive, or neutral)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "43pVRccMT0bT"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class classifier(nn.Module):\n",
        "    \n",
        "    # Define all the layers used in model\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, dropout):\n",
        "        \n",
        "        super().__init__()          \n",
        "        \n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        \n",
        "        # LSTM layer\n",
        "        self.encoder = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers=n_layers, \n",
        "                           dropout=dropout,\n",
        "                           batch_first=True)\n",
        "        # try using nn.GRU or nn.RNN here and compare their performances\n",
        "        # try bidirectional and compare their performances\n",
        "        \n",
        "        # Dense layer\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        # text = [batch size, sent_length]\n",
        "        embedded = self.embedding(text)\n",
        "        # embedded = [batch size, sent_len, emb dim]\n",
        "      \n",
        "        # packed sequence\n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths.cpu(), batch_first=True)\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.encoder(packed_embedded)\n",
        "        #hidden = [batch size, num layers * num directions,hid dim]\n",
        "        #cell = [batch size, num layers * num directions,hid dim]\n",
        "    \n",
        "        # Hidden = [batch size, hid dim * num directions]\n",
        "        dense_outputs = self.fc(hidden)   \n",
        "        \n",
        "        # Final activation function softmax\n",
        "        output = F.softmax(dense_outputs[0], dim=1)\n",
        "            \n",
        "        return output"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwBoGE_X_Fl8"
      },
      "source": [
        "# Define hyperparameters\n",
        "size_of_vocab = len(sentence.vocab)\n",
        "embedding_dim = 300\n",
        "num_hidden_nodes = 100\n",
        "num_output_nodes = 3\n",
        "num_layers = 2\n",
        "dropout = 0.2\n",
        "\n",
        "# Instantiate the model\n",
        "model = classifier(size_of_vocab, embedding_dim, num_hidden_nodes, num_output_nodes, num_layers, dropout = dropout)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-pOMqzJ3eTv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66531598-a713-40a8-f60e-d3f1ac611be4"
      },
      "source": [
        "print(model)\n",
        "\n",
        "#No. of trianable parameters\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "classifier(\n",
            "  (embedding): Embedding(26545, 300)\n",
            "  (encoder): LSTM(300, 100, num_layers=2, batch_first=True, dropout=0.2)\n",
            "  (fc): Linear(in_features=100, out_features=3, bias=True)\n",
            ")\n",
            "The model has 8,205,403 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eXajorf5Xz7t"
      },
      "source": [
        "## Model Training and Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrE9RpMtZ1Vs"
      },
      "source": [
        "First define the optimizer and loss functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u86JWdlXvu5"
      },
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# define optimizer and loss\n",
        "optimizer = optim.Adam(model.parameters(), lr=2e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# define metric\n",
        "def binary_accuracy(preds, y):\n",
        "    #round predictions to the closest integer\n",
        "    _, predictions = torch.max(preds, 1)\n",
        "    \n",
        "    correct = (predictions == y).float() \n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "    \n",
        "# push to cuda if available\n",
        "model = model.to(device)\n",
        "criterion = criterion.to(device)"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3VCJtNb3Zt8w"
      },
      "source": [
        "The main thing to be aware of in this new training loop is that we have to reference `batch.tweets` and `batch.labels` to get the particular fields we’re interested in; they don’t fall out quite as nicely from the enumerator as they do in torchvision."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WjEPLKsAiS_"
      },
      "source": [
        "**Training Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HDWNnGK3Y5oJ"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.tweets   \n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZcHhkkvAsCt"
      },
      "source": [
        "**Evaluation Loop**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHEe-zSVAriL"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6LJFW7HaJoV"
      },
      "source": [
        "**Let's Train and Evaluate**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tq330XlnaEU9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb5ec004-5749-492e-dccd-873c502468d3"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\tTrain Loss: 0.915 | Train Acc: 68.00%\n",
            "\t Val. Loss: 0.836 |  Val. Acc: 72.34% \n",
            "\n",
            "\tTrain Loss: 0.838 | Train Acc: 71.99%\n",
            "\t Val. Loss: 0.833 |  Val. Acc: 72.34% \n",
            "\n",
            "\tTrain Loss: 0.835 | Train Acc: 72.01%\n",
            "\t Val. Loss: 0.832 |  Val. Acc: 72.34% \n",
            "\n",
            "\tTrain Loss: 0.833 | Train Acc: 72.03%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.34% \n",
            "\n",
            "\tTrain Loss: 0.831 | Train Acc: 72.12%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.34% \n",
            "\n",
            "\tTrain Loss: 0.830 | Train Acc: 72.32%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.23% \n",
            "\n",
            "\tTrain Loss: 0.828 | Train Acc: 72.51%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.23% \n",
            "\n",
            "\tTrain Loss: 0.827 | Train Acc: 72.79%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.28% \n",
            "\n",
            "\tTrain Loss: 0.826 | Train Acc: 72.91%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.23% \n",
            "\n",
            "\tTrain Loss: 0.824 | Train Acc: 73.04%\n",
            "\t Val. Loss: 0.831 |  Val. Acc: 72.23% \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZgzB0ZkHVTI"
      },
      "source": [
        "## Model Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZZfnWo0abRx"
      },
      "source": [
        "#load weights and tokenizer\n",
        "\n",
        "path='./saved_weights.pt'\n",
        "model.load_state_dict(torch.load(path));\n",
        "model.eval();\n",
        "tokenizer_file = open('./tokenizer.pkl', 'rb')\n",
        "tokenizer = pickle.load(tokenizer_file)\n",
        "\n",
        "#inference \n",
        "\n",
        "import spacy\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def classify_tweet(tweet):\n",
        "    \n",
        "    categories = {0: \"Negative\", 1:\"Positive\", 2:\"Neutral\"}\n",
        "    \n",
        "    # tokenize the tweet \n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(tweet)] \n",
        "    # convert to integer sequence using predefined tokenizer dictionary\n",
        "    indexed = [tokenizer[t] for t in tokenized]        \n",
        "    # compute no. of words        \n",
        "    length = [len(indexed)]\n",
        "    # convert to tensor                                    \n",
        "    tensor = torch.LongTensor(indexed).to(device)   \n",
        "    # reshape in form of batch, no. of words           \n",
        "    tensor = tensor.unsqueeze(1).T  \n",
        "    # convert to tensor                          \n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    # Get the model prediction                  \n",
        "    prediction = model(tensor, length_tensor)\n",
        "\n",
        "    _, pred = torch.max(prediction, 1) \n",
        "    \n",
        "    return categories[pred.item()]"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTkHLEipIlM9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2288c38c-8a1b-4e83-dca7-6681ac664a2d"
      },
      "source": [
        "classify_tweet(\"Take Care of My Cat offers a refreshingly different slice of Asian cinema \")"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bPMGLoys6MH5",
        "outputId": "584c3393-c352-46da-a061-a73d52dfc08c"
      },
      "source": [
        "classify_tweet(\"This is a great record\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Negative'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVjCuKK_LVEF"
      },
      "source": [
        "## Discussion on Data Augmentation Techniques \n",
        "\n",
        "You might wonder exactly how you can augment text data. After all, you can’t really flip it horizontally as you can an image! :D \n",
        "\n",
        "In contrast to data augmentation in images, augmentation techniques on data is very specific to final product you are building. As its general usage on any type of textual data doesn't provides a significant performance boost, that's why unlike torchvision, torchtext doesn’t offer a augmentation pipeline. Due to powerful models as transformers, augmentation tecnhiques are not so preferred now-a-days. But its better to know about some techniques with text that will provide your model with a little more information for training. \n",
        "\n",
        "### Synonym Replacement\n",
        "\n",
        "First, you could replace words in the sentence with synonyms, like so:\n",
        "\n",
        "    The dog slept on the mat\n",
        "\n",
        "could become\n",
        "\n",
        "    The dog slept on the rug\n",
        "\n",
        "Aside from the dog's insistence that a rug is much softer than a mat, the meaning of the sentence hasn’t changed. But mat and rug will be mapped to different indices in the vocabulary, so the model will learn that the two sentences map to the same label, and hopefully that there’s a connection between those two words, as everything else in the sentences is the same."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_uEfWJpL6Nq"
      },
      "source": [
        "### Random Insertion\n",
        "A random insertion technique looks at a sentence and then randomly inserts synonyms of existing non-stopwords into the sentence n times. Assuming you have a way of getting a synonym of a word and a way of eliminating stopwords (common words such as and, it, the, etc.), shown, but not implemented, in this function via get_synonyms() and get_stopwords(), an implementation of this would be as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Alm5D7WIvAC"
      },
      "source": [
        "def random_insertion(sentence, n): \n",
        "    words = remove_stopwords(sentence) \n",
        "    for _ in range(n):\n",
        "        new_synonym = get_synonyms(random.choice(words))\n",
        "        sentence.insert(randrange(len(sentence)+1), new_synonym) \n",
        "    return sentence"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqLWzwJ3Mm8h"
      },
      "source": [
        "## Random Deletion\n",
        "As the name suggests, random deletion deletes words from a sentence. Given a probability parameter p, it will go through the sentence and decide whether to delete a word or not based on that random probability. Consider of it as pixel dropouts while treating images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7Dz7JJfMqyC"
      },
      "source": [
        "def random_deletion(words, p=0.5): \n",
        "    if len(words) == 1: # return if single word\n",
        "        return words\n",
        "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
        "    if len(remaining) == 0: # if not left, sample a random word\n",
        "        return [random.choice(words)] \n",
        "    else:\n",
        "        return remaining"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOIbi5WzO5OU"
      },
      "source": [
        "### Random Swap\n",
        "The random swap augmentation takes a sentence and then swaps words within it n times, with each iteration working on the previously swapped sentence. Here we sample two random numbers based on the length of the sentence, and then just keep swapping until we hit n."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnkbG15HO3Yj"
      },
      "source": [
        "def random_swap(sentence, n=5):\n",
        "  new_sent = sentence.split(' ')\n",
        "  #print(new_sent)\n",
        "  length = range(len(new_sent))\n",
        "  #print(length) \n",
        "  for _ in range(n):\n",
        "      idx1, idx2 = random.sample(length, 2)\n",
        "      #print(idx1, idx2)\n",
        "      new_sent[idx1], new_sent[idx2] = new_sent[idx2], new_sent[idx1]\n",
        "      listToStr = ' '.join([str(elem) for elem in new_sent])  \n",
        "  return listToStr"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599NpwfMR5Vm"
      },
      "source": [
        "For more on this please go through this [paper](https://arxiv.org/pdf/1901.11196.pdf)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5aeKuNCRGip"
      },
      "source": [
        "### Back Translation\n",
        "\n",
        "Another popular approach for augmenting text datasets is back translation. This involves translating a sentence from our target language into one or more other languages and then translating all of them back to the original language. We can use the Python library googletrans for this purpose. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WsC0C2g-aIp"
      },
      "source": [
        "#!pip uninstall googletrans\n",
        "#!git clone https://github.com/BoseCorp/py-googletrans.git\n"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aSalmYj_W2n",
        "outputId": "363a58a0-5b46-4fc9-f6ce-7eb3b2a3e760"
      },
      "source": [
        "!pip install googletrans==3.1.0a0"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting googletrans==3.1.0a0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/3d/4e3a1609bf52f2f7b00436cc751eb977e27040665dde2bd57e7152989672/googletrans-3.1.0a0.tar.gz\n",
            "Collecting httpx==0.13.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/b4/698b284c6aed4d7c2b4fe3ba5df1fcf6093612423797e76fbb24890dd22f/httpx-0.13.3-py3-none-any.whl (55kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25hCollecting rfc3986<2,>=1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/c4/e5/63ca2c4edf4e00657584608bee1001302bbf8c5f569340b78304f2f446cb/rfc3986-1.5.0-py2.py3-none-any.whl\n",
            "Collecting hstspreload\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/50/606213e12fb49c5eb667df0936223dcaf461f94e215ea60244b2b1e9b039/hstspreload-2020.12.22-py3-none-any.whl (994kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna==2.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
            "Requirement already satisfied: chardet==3.* in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
            "Collecting sniffio\n",
            "  Downloading https://files.pythonhosted.org/packages/52/b0/7b2e028b63d092804b6794595871f936aafa5e9322dcaaad50ebf67445b3/sniffio-1.2.0-py3-none-any.whl\n",
            "Collecting httpcore==0.9.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dd/d5/e4ff9318693ac6101a2095e580908b591838c6f33df8d3ee8dd953ba96a8/httpcore-0.9.1-py3-none-any.whl (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: certifi in /usr/local/lib/python3.7/dist-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2020.12.5)\n",
            "Collecting h11<0.10,>=0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5a/fd/3dad730b0f95e78aeeb742f96fa7bbecbdd56a58e405d3da440d5bfb90c6/h11-0.9.0-py2.py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 11.0MB/s \n",
            "\u001b[?25hCollecting h2==3.*\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/de/da019bcc539eeab02f6d45836f23858ac467f584bfec7a526ef200242afe/h2-3.2.0-py2.py3-none-any.whl (65kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.1MB/s \n",
            "\u001b[?25hCollecting hpack<4,>=3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/8a/cc/e53517f4a1e13f74776ca93271caef378dadec14d71c61c949d759d3db69/hpack-3.0.0-py2.py3-none-any.whl\n",
            "Collecting hyperframe<6,>=5.2.0\n",
            "  Downloading https://files.pythonhosted.org/packages/19/0c/bf88182bcb5dce3094e2f3e4fe20db28a9928cb7bd5b08024030e4b140db/hyperframe-5.2.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: googletrans\n",
            "  Building wheel for googletrans (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for googletrans: filename=googletrans-3.1.0a0-cp37-none-any.whl size=16368 sha256=73dfb2e52d77b70d78cecd3e32440eb31d9d545e64cd69eb9ffb679202b619f7\n",
            "  Stored in directory: /root/.cache/pip/wheels/27/7a/a0/aff3babbb775549ce6813cb8fa7ff3c0848c4dc62c20f8fdac\n",
            "Successfully built googletrans\n",
            "Installing collected packages: rfc3986, hstspreload, sniffio, h11, hpack, hyperframe, h2, httpcore, httpx, googletrans\n",
            "Successfully installed googletrans-3.1.0a0 h11-0.9.0 h2-3.2.0 hpack-3.0.0 hstspreload-2020.12.22 httpcore-0.9.1 httpx-0.13.3 hyperframe-5.2.0 rfc3986-1.5.0 sniffio-1.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHhNBbYrRXNy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad2ccf07-41e4-4a17-8361-a2f2f0f9ad0d"
      },
      "source": [
        "\n",
        "#import random\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "translator = Translator()\n",
        "sentence = ['The dog slept on the rug']\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "#trans_lang = 'ja'\n",
        "print(f\"Translating to {googletrans.LANGUAGES[trans_lang]}\")\n",
        "\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "t_text = [t.text for t in translations]\n",
        "print(t_text)\n",
        "\n",
        "translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "en_text = [t.text for t in translations_en_random]\n",
        "print(en_text)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Translating to armenian\n",
            "['Շունը քնում էր գորգի վրա']\n",
            "['The dog was sleeping on the carpet']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sNyGtwAwt9se",
        "outputId": "20e9da30-9f1e-41c5-eba3-15fd71e22644"
      },
      "source": [
        "#testing\n",
        "import random\n",
        "import googletrans\n",
        "from googletrans import Translator\n",
        "\n",
        "available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "trans_lang = random.choice(available_langs) \n",
        "\n",
        "translator = Translator()\n",
        "sentence = ['The dog slept on the rug']\n",
        "translations = translator.translate(sentence, dest=trans_lang) \n",
        "type(translations)\n",
        "\n",
        "t_text = []\n",
        "for t in iter(translations):\n",
        "  print(t.text)\n",
        "  t_text.append(t.text)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "הכלב ישן על השטיח\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G1jBhR8zCjNw"
      },
      "source": [
        "def back_translate(sentence):\n",
        "  translator = Translator()\n",
        "  available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs)\n",
        "  translations = translator.translate(sentence, dest=trans_lang)\n",
        "  print(translations)\n",
        "  #print(dir(translations))\n",
        "  t_text = []\n",
        "  for t in iter(translations):\n",
        "    print(t.text)\n",
        "    t_text.append(t.text)\n",
        "  #t_text = [t.text for t in translations]\n",
        "  translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "  en_text = [t.text for t in translations_en_random]\n",
        "  return en_text\n",
        "\n"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vUs9YbY4C-3t",
        "outputId": "37d9cbb7-417f-4f2b-e4d2-c2691110f4a7"
      },
      "source": [
        "sentence = ['The dog slept on the rug']\n",
        "back_translate(sentence)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[<googletrans.models.Translated object at 0x7f55483a5f90>]\n",
            "Anjing bobo dina karpét\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The dog sleeps on the carpet']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fupKZ1sAEao4",
        "outputId": "c9389168-df37-45bf-baf3-785b9583581c"
      },
      "source": [
        "sentence = ['The dog slept on the rug']\n",
        "random_deletion(sentence, p=0)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['The dog slept on the rug']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wZPWZ8caWumI",
        "outputId": "5dc8e58a-b1b9-4739-c432-614d05657672"
      },
      "source": [
        "sentence1 = 'The dog slept on the rug'\n",
        "print(random_swap(sentence1))"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the on The dog slept rug\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u6tT_PlMCQMb",
        "outputId": "6aa40c6f-6285-4553-e346-7c8f15d9c7d5"
      },
      "source": [
        "pip install -U nltk"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting nltk\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/37/9532ddd4b1bbb619333d5708aaad9bf1742f051a664c3c6fa6632a105fd8/nltk-3.6.2-py3-none-any.whl (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 23.0MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 30.9MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 21.2MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 16.7MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 8.5MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 7.8MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 8.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 9.7MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 10.2MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 8.4MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 8.4MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 8.4MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 8.4MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 8.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 348kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 358kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 368kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 378kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 389kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 399kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 419kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▌                      | 430kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 440kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████                      | 450kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 460kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 471kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 686kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 696kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 706kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 716kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 737kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 747kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 757kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 768kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 778kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 788kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 798kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 808kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 829kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 839kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 849kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 860kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 870kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▍            | 880kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 890kB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 901kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 921kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 931kB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 942kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 952kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.2MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.3MB 8.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.4MB 8.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: regex in /usr/local/lib/python3.7/dist-packages (from nltk) (2019.12.20)\n",
            "Installing collected packages: nltk\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "Successfully installed nltk-3.6.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0nyqqCBeDrN9",
        "outputId": "c3d4631c-343f-4fdc-ce19-c60031c95381"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ejM3NTvD31W",
        "outputId": "28c14f03-dc9f-4ed0-d832-4695678bb6ef"
      },
      "source": [
        "#!python eda_nlp/code/augment.py --input=tweets.csv  --output=tweets_augmented.txt \n",
        "!python eda_nlp/code/augment.py --input=tweets.csv  --output=tweets_augmented.txt --num_aug=16 --alpha_sr=0.05 --alpha_rd=0.1 --alpha_ri=0.0 --alpha_rs=0.0"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "python3: can't open file 'eda_nlp/code/augment.py': [Errno 2] No such file or directory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ETagey8wXKnW",
        "outputId": "221445e0-f328-43ec-8fc9-8b88497fb078"
      },
      "source": [
        "  !git clone https://github.com/jasonwei20/eda_nlp.git"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'eda_nlp'...\n",
            "remote: Enumerating objects: 396, done.\u001b[K\n",
            "remote: Counting objects: 100% (17/17), done.\u001b[K\n",
            "remote: Compressing objects: 100% (13/13), done.\u001b[K\n",
            "remote: Total 396 (delta 6), reused 8 (delta 4), pack-reused 379\u001b[K\n",
            "Receiving objects: 100% (396/396), 20.42 MiB | 21.44 MiB/s, done.\n",
            "Resolving deltas: 100% (189/189), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWqEJosjVMEh",
        "outputId": "1662f0e6-91d0-40e5-d36b-bf41e42f88d8"
      },
      "source": [
        "\n",
        "def read_text():\n",
        "    ifname = 'content/SOStr.txt'\n",
        "    lines = open(ifname, 'r').read().split('\\n')\n",
        "\n",
        "    texts = []\n",
        "    for line in lines:\n",
        "        params = line.split('|')\n",
        "        if len(params) > 1:\n",
        "            text = ' '.join(params)\n",
        "            texts.append(text)\n",
        "\n",
        "    return texts\n",
        "texts=read_text()\n",
        "len(texts)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m9Vee_QrVNXh",
        "outputId": "16af6535-06ca-4bc9-9234-5994598f6183"
      },
      "source": [
        "def read_splitlabel():\n",
        "    ifname = 'content/datasetSplit.txt'\n",
        "    lines = open(ifname, 'r').read().split('\\n')\n",
        "\n",
        "    splitlabels = []\n",
        "    for line in lines[1:]:\n",
        "        params = line.split(',')\n",
        "        if len(params) == 2:\n",
        "            splitlabels.append(int(params[1]))\n",
        "    \n",
        "    return splitlabels\n",
        "splitlabels=read_splitlabel()\n",
        "len(splitlabels)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11855"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sCbf5GjxVR1g",
        "outputId": "81c1003f-e887-4280-aa1c-4a8adef6e5f7"
      },
      "source": [
        "def read_sentiscore():\n",
        "\tifname = 'content/sentiment_labels.txt'\n",
        "\tlines = open(ifname, 'r').read().split('\\n')\n",
        "\n",
        "\tsentiscores = []\n",
        "\tfor line in lines[1:]:\n",
        "\t\tparams = line.split('|')\n",
        "\t\tif len(params) == 2:\n",
        "\t\t\tsentiscores.append(float(params[1]))\n",
        "\n",
        "\treturn sentiscores\n",
        "sentiscores=read_sentiscore()\n",
        "len(sentiscores)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "239232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u9KNUxBBVUvo",
        "outputId": "77dd840c-c204-4073-c4fc-06ad78439905"
      },
      "source": [
        "def read_phraseid():\n",
        "    ifname = 'content/dictionary.txt'\n",
        "    lines = open(ifname, 'r').read().split('\\n')\n",
        "\n",
        "    phraseid = {}\n",
        "    for line in lines:\n",
        "        params = line.split('|')\n",
        "        if len(params) == 2:\n",
        "            phraseid[params[0]] = int(params[1])\n",
        "\n",
        "    return phraseid\n",
        "phraseid=read_phraseid()\n",
        "len(phraseid)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "239232"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uK-MS5w0VdyX"
      },
      "source": [
        "def prepare_valence():\n",
        "    texts = read_text()\n",
        "    splitlabels = read_splitlabel()\n",
        "    sentiscores = read_sentiscore()\n",
        "    phraseid = read_phraseid()\n",
        "\n",
        "    train_text = []\n",
        "    train_label = []\n",
        "    \n",
        "    valid_text = []\n",
        "    valid_label = []\n",
        "\n",
        "    test_text = []\n",
        "    test_label = []\n",
        "\n",
        "    n_sample = len(texts)\n",
        "    if n_sample == len(splitlabels) and len(sentiscores) == len(phraseid):\n",
        "        print('%d samples'%(n_sample))\n",
        "    else:\n",
        "        print('reading fail')\n",
        "\n",
        "    for i, didx in enumerate(splitlabels):\n",
        "        if didx == 1:\n",
        "            list_text = train_text\n",
        "            list_label = train_label\n",
        "        elif didx == 3:\n",
        "            list_text = valid_text\n",
        "            list_label = valid_label\n",
        "        elif didx == 2:\n",
        "            list_text = test_text\n",
        "            list_label = test_label\n",
        "\n",
        "        list_text.append(texts[i])\n",
        "        list_label.append(sentiscores[phraseid[texts[i]]])\n",
        "        \n",
        "    return train_text,train_label,test_text,test_label,valid_text,valid_label"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uJ5v-6EHVg2U"
      },
      "source": [
        "\n",
        "def labelize(text,label):\n",
        "        y = []\n",
        "        for l in label:\n",
        "            if l <= 0.2:\n",
        "                y.append(0)\n",
        "            elif l <= 0.4:\n",
        "                y.append(1)\n",
        "            elif l <= 0.6:\n",
        "                y.append(2)\n",
        "            elif l <= 0.8:\n",
        "                y.append(3)\n",
        "            else:\n",
        "                y.append(4)\n",
        "        print(len(y))\n",
        "        return (text, y)\n"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PcMU1gkkWMJ6",
        "outputId": "c918e1cd-92ee-4b12-c540-8b310f394f12"
      },
      "source": [
        "train_tx,train_l,test_tx,test_l,valid_tx,valid_l=prepare_valence()\n",
        "train_text,train_label=labelize(train_tx,train_l)    \n",
        "\n",
        "test_text,test_label=labelize(test_tx,test_l)\n",
        "valid_text,valid_label=labelize(valid_tx,valid_l)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11855 samples\n",
            "8544\n",
            "2210\n",
            "1101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D9zU7lnt-bKQ",
        "outputId": "2306adcc-ca92-49f9-cffb-3f637c3dd5a7"
      },
      "source": [
        "(len(train_tx), len(valid_tx))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(8544, 1101)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoFDN1p-g57W",
        "outputId": "95c944c3-8238-457a-eab0-8bfda30cfa36"
      },
      "source": [
        "print(type(train_tx))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'list'>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4JE62vnfoCJ",
        "outputId": "4d151d45-674a-42cf-fb3f-ce6d141271e8"
      },
      "source": [
        "len(train_tx)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "8544"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rwpirs5SXsfd"
      },
      "source": [
        "Below failing due to googletrans API limits"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "tFZurz9fGr4a",
        "outputId": "3d58ceb1-385a-419b-ff2c-4b9e19998b43"
      },
      "source": [
        "new_train_tx = []\n",
        "count = 0 \n",
        "for tweet in train_tx:\n",
        "  translator = Translator()\n",
        "  available_langs = list(googletrans.LANGUAGES.keys()) \n",
        "  trans_lang = random.choice(available_langs)\n",
        "  translations = translator.translate(sentence, dest=trans_lang)\n",
        "  t_text = []\n",
        "  for t in iter(translations):\n",
        "    #print(t.text)\n",
        "    t_text.append(t.text)\n",
        "  #t_text = [t.text for t in translations]\n",
        "  translations_en_random = translator.translate(t_text, src=trans_lang, dest='en') \n",
        "  tweet = [t.text for t in translations_en_random]\n",
        "  #tweet = back_translate(tweet)\n",
        "  #print(\"Tweet after tranlate: \", tweet)\n",
        "  tweet = random_swap(tweet[0])\n",
        "  #print(\"tweet after swap: \",tweet)\n",
        "  tweet = random_deletion(tweet, p=0.5)\n",
        "  #print(\"tweet after delete: \",tweet)\n",
        "  new_train_tx.append(tweet)\n",
        "  count += 1\n",
        "  if count%10 ==0:\n",
        "    print(\"count: \", count)"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "count:  10\n",
            "count:  20\n",
            "count:  30\n",
            "count:  40\n",
            "count:  50\n",
            "count:  60\n",
            "count:  70\n",
            "count:  80\n",
            "count:  90\n",
            "count:  100\n",
            "count:  110\n",
            "count:  120\n",
            "count:  130\n",
            "count:  140\n",
            "count:  150\n",
            "count:  160\n",
            "count:  170\n",
            "count:  180\n",
            "count:  190\n",
            "count:  200\n",
            "count:  210\n",
            "count:  220\n",
            "count:  230\n",
            "count:  240\n",
            "count:  250\n",
            "count:  260\n",
            "count:  270\n",
            "count:  280\n",
            "count:  290\n",
            "count:  300\n",
            "count:  310\n",
            "count:  320\n",
            "count:  330\n",
            "count:  340\n",
            "count:  350\n",
            "count:  360\n",
            "count:  370\n",
            "count:  380\n",
            "count:  390\n",
            "count:  400\n",
            "count:  410\n",
            "count:  420\n",
            "count:  430\n",
            "count:  440\n",
            "count:  450\n",
            "count:  460\n",
            "count:  470\n",
            "count:  480\n",
            "count:  490\n",
            "count:  500\n",
            "count:  510\n",
            "count:  520\n",
            "count:  530\n",
            "count:  540\n",
            "count:  550\n",
            "count:  560\n",
            "count:  570\n",
            "count:  580\n",
            "count:  590\n",
            "count:  600\n",
            "count:  610\n",
            "count:  620\n",
            "count:  630\n",
            "count:  640\n",
            "count:  650\n",
            "count:  660\n",
            "count:  670\n",
            "count:  680\n",
            "count:  690\n",
            "count:  700\n",
            "count:  710\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ProtocolError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2/connection.py\u001b[0m in \u001b[0;36mprocess_input\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transitions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: (<ConnectionState.CLOSED: 3>, <ConnectionInputs.RECV_PING: 14>)",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mProtocolError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-64-e2db84f2c9f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mt_text\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m   \u001b[0;31m#t_text = [t.text for t in translations]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m   \u001b[0mtranslations_en_random\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrans_lang\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'en'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m   \u001b[0mtweet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtranslations_en_random\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m   \u001b[0;31m#tweet = back_translate(tweet)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                 \u001b[0mtranslated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslated\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36mtranslate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0morigin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_translate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# this code will be updated when the format is changed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/googletrans/client.py\u001b[0m in \u001b[0;36m_translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m         \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRANSLATE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pick_service_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, url, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    761\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    762\u001b[0m             \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 763\u001b[0;31m             \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    764\u001b[0m         )\n\u001b[1;32m    765\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, data, files, json, params, headers, cookies, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    599\u001b[0m         )\n\u001b[1;32m    600\u001b[0m         return self.send(\n\u001b[0;32m--> 601\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    602\u001b[0m         )\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, allow_redirects, timeout)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m         response = self.send_handling_redirects(\n\u001b[0;32m--> 621\u001b[0;31m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m         )\n\u001b[1;32m    623\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_handling_redirects\u001b[0;34m(self, request, auth, timeout, allow_redirects, history)\u001b[0m\n\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m             response = self.send_handling_auth(\n\u001b[0;32m--> 648\u001b[0;31m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    649\u001b[0m             )\n\u001b[1;32m    650\u001b[0m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_handling_auth\u001b[0;34m(self, request, history, auth, timeout)\u001b[0m\n\u001b[1;32m    682\u001b[0m         \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauth_flow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 684\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    685\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_response_body\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    686\u001b[0m                 \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend_single_request\u001b[0;34m(self, request, timeout)\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             )\n\u001b[1;32m    721\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                 response = connection.request(\n\u001b[0;32m--> 153\u001b[0;31m                     \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m                 )\n\u001b[1;32m    155\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mNewConnectionRequired\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;34m\"connection.request method=%r url=%r headers=%r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         )\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_open_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTimeoutDict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mSyncSocketStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    116\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstreams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh2_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mh2_stream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_streams_semaphore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, headers, stream, timeout)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m         \u001b[0;31m# Receive the response.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m         \u001b[0mreason_phrase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_reason_phrase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m         stream = SyncByteStream(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mreceive_response\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    342\u001b[0m         \"\"\"\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponseReceived\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mwait_for_event\u001b[0;34m(self, stream_id, timeout)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevents\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstream_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/httpcore/_sync/http2.py\u001b[0m in \u001b[0;36mreceive_events\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh2_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreceive_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mevents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m             \u001b[0mevent_stream_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"stream_id\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2/connection.py\u001b[0m in \u001b[0;36mreceive_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1461\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1462\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mframe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mincoming_buffer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1463\u001b[0;31m                 \u001b[0mevents\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1464\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidPaddingError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1465\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrorCodes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPROTOCOL_ERROR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2/connection.py\u001b[0m in \u001b[0;36m_receive_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m   1484\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1485\u001b[0m             \u001b[0;31m# I don't love using __class__ here, maybe reconsider it.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1486\u001b[0;31m             \u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_frame_dispatch_table\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1487\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStreamClosedError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1488\u001b[0m             \u001b[0;31m# If the stream was closed by RST_STREAM, we just send a RST_STREAM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2/connection.py\u001b[0m in \u001b[0;36m_receive_ping_frame\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m   1759\u001b[0m         \"\"\"\n\u001b[1;32m   1760\u001b[0m         events = self.state_machine.process_input(\n\u001b[0;32m-> 1761\u001b[0;31m             \u001b[0mConnectionInputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRECV_PING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1762\u001b[0m         )\n\u001b[1;32m   1763\u001b[0m         \u001b[0mflags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/h2/connection.py\u001b[0m in \u001b[0;36mprocess_input\u001b[0;34m(self, input_)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnectionState\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCLOSED\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             raise ProtocolError(\n\u001b[0;32m--> 229\u001b[0;31m                 \u001b[0;34m\"Invalid input %s in state %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mold_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m             )\n\u001b[1;32m    231\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mProtocolError\u001b[0m: Invalid input ConnectionInputs.RECV_PING in state ConnectionState.CLOSED"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zy9Ct3BLAgFk"
      },
      "source": [
        "# Training Loop Modified"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zRDekWJ9YtxU"
      },
      "source": [
        "def random_swap(sentence, n=5): \n",
        "    length = range(len(sentence)) \n",
        "    for _ in range(n):\n",
        "        idx1, idx2 = random.sample(length, 2)\n",
        "        sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
        "    return sentence"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E22zOUpoBi5W"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    # initialize every epoch \n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    # set the model in training phase\n",
        "    model.train()  \n",
        "    \n",
        "    for batch in iterator:\n",
        "        \n",
        "        # resets the gradients after every batch\n",
        "        optimizer.zero_grad()   \n",
        "        \n",
        "        \n",
        "        # retrieve text and no. of words\n",
        "        tweet, tweet_lengths = batch.tweets\n",
        "\n",
        "        # randomly deleting the words in the sentence\n",
        "        #tweet = back_translate(tweet)\n",
        "        print(tweet)\n",
        "        tweet = random_swap(tweet)\n",
        "        print(\"tweet after swap\",tweet)\n",
        "        tweet = random_deletion(tweet, p=0.5)\n",
        "        print(\"tweet after delete\",tweet)\n",
        "        \n",
        "\n",
        "        \n",
        "        # convert to 1D tensor\n",
        "        predictions = model(tweet, tweet_lengths).squeeze()  \n",
        "        \n",
        "        # compute the loss\n",
        "        loss = criterion(predictions, batch.labels)        \n",
        "        \n",
        "        # compute the binary accuracy\n",
        "        acc = binary_accuracy(predictions, batch.labels)   \n",
        "        \n",
        "        # backpropage the loss and compute the gradients\n",
        "        loss.backward()       \n",
        "        \n",
        "        # update the weights\n",
        "        optimizer.step()      \n",
        "        \n",
        "        # loss and accuracy\n",
        "        epoch_loss += loss.item()  \n",
        "        epoch_acc += acc.item()    \n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCl2KJz1YZ76"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6Mcc_kIYM9m"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    # initialize every epoch\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    # deactivating dropout layers\n",
        "    model.eval()\n",
        "    \n",
        "    # deactivates autograd\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for batch in iterator:\n",
        "        \n",
        "            # retrieve text and no. of words\n",
        "            tweet, tweet_lengths = batch.tweets\n",
        "            \n",
        "            # convert to 1d tensor\n",
        "            predictions = model(tweet, tweet_lengths).squeeze()\n",
        "            \n",
        "            # compute loss and accuracy\n",
        "            loss = criterion(predictions, batch.labels)\n",
        "            acc = binary_accuracy(predictions, batch.labels)\n",
        "            \n",
        "            # keep track of loss and accuracy\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJFG5hpQYfzI"
      },
      "source": [
        "# Train and Evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrPR1NR-Ycuf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b6e7f42c-6e27-42e3-c64e-4b8bfa38413b"
      },
      "source": [
        "N_EPOCHS = 10\n",
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "     \n",
        "    # train the model\n",
        "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
        "    \n",
        "    # evaluate the model\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    # save the best model\n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
        "    \n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}% \\n')"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[   38,  1812,  3197,     9,    38,    90,  2694,     6,   113,     2,\n",
            "           203, 10311,     4,     2,    60],\n",
            "        [   29,     3,  3773,     4, 13172,  5976,    33,     2, 20829,  1025,\n",
            "          5699,     2,  3827,   750,  7607],\n",
            "        [    2, 23562,  1314,    56, 16075,    61,   108,   358,    53,    32,\n",
            "            64,  2563,    13, 13327,  1554],\n",
            "        [ 2081,  1350,     7, 24801,    10,  4530,  4582,   598,    23,   241,\n",
            "            11,    40,    10,   172,    20],\n",
            "        [   10,   158,   881,    46,    20,   245,     2, 14250,  1678,     5,\n",
            "             2,  7726, 23250,   211,    36],\n",
            "        [   39,  1828,    50,  5438,     3,    47,   410,    70,     6,   330,\n",
            "             2,   183, 22673,    12,  1387],\n",
            "        [   10,    73,  7698,   938, 10329,    15,  2802,    24,     3,   451,\n",
            "             9,    55,    34,    21,  3479],\n",
            "        [    2,  1400,  4025,   242,  3539,  1920,     6, 22532,   124,     2,\n",
            "          9071,     4,     9,  5943,  2922],\n",
            "        [    2,   972,  8126,    12,   536,  1880,     7, 25094,    34,     3,\n",
            "           449,     8,    14,   109,   207],\n",
            "        [    3,  3038,    17,    30,     3,   748, 13996,     8,    14, 12030,\n",
            "         15122,     8,    14, 13751, 26303],\n",
            "        [   16, 19267,  2950,     5,   383,   583,   582,    17,    26,     2,\n",
            "           327,  1265,     4,     2, 15697],\n",
            "        [   16,   448,  1845,     4,    28,     4,     2,   100,   123,   134,\n",
            "         17813,    86,    10,    34,    21],\n",
            "        [   37,    10,   248,   634,     6,  1728,     5,  4352,    22,    11,\n",
            "            14,  1205,   753,  1993,  1309],\n",
            "        [   36,     9,    39,  3106,  1073,    50,   312,     7,  1553,     8,\n",
            "             2,   786, 16765,    44,   665],\n",
            "        [  188,  2482, 12244,  5572,     2, 15193,     4,     3,   352,   939,\n",
            "         20127,    44,     7,     3,  4281],\n",
            "        [   45,    14, 23576,  2029,    54,  5305,     8, 19394,     3,   161,\n",
            "            29,    33,     2,   620,  1331],\n",
            "        [ 5131,   923,     7,    48,   169,    30,   724,   362, 10558,     4,\n",
            "          3035,     5, 13005,     2,  9766],\n",
            "        [    2,   364,     7,  2151, 19685,  8086,     5,  8086,     4, 12266,\n",
            "             4,    10,   472,     6,   421],\n",
            "        [    3,  2934,  1189,    13, 17292,     3, 15675,  7551,     9,    46,\n",
            "            20,   485,  4596,     2,   141],\n",
            "        [   40,    19,    55,   113,   417,     2,  7986,   246, 18068,    56,\n",
            "            21,   104,   202,    69,    62],\n",
            "        [    3, 18389,   256,     4, 14094,     9,  1046,     6,  1721,   194,\n",
            "           124,    11, 15644,  1212,   259],\n",
            "        [  178,  3477,     8,     3,    58, 25774,    10,     7,    16,   971,\n",
            "            17,    13,     2,    57,   322],\n",
            "        [    6, 19556,    56,  3134,   103,     9,  1685,  2168,   125,   283,\n",
            "             7,   132,  1044,     5,  4826],\n",
            "        [   31,     3,  2257,  2880,  4067,   953,   941,    44, 17625,    49,\n",
            "          7984,     5,    34,    70,  3599],\n",
            "        [    8,     2, 10101, 17981,    17,  2204,   778,  1776,     8,     2,\n",
            "         20134,    21,    16,  8428,  3521],\n",
            "        [    2,   290,   226,  5082,     2,   462,    27,     2,  2429,     4,\n",
            "           165,   695,   200,   992,   676],\n",
            "        [   10,    96,   230,   224,     6,   113,  7075,     5,   204,   115,\n",
            "            15,    18,    12,   177,  2645],\n",
            "        [  137,  3346, 25895,   257,   233,   402, 22591,  5499,    74,    32,\n",
            "            29,  7627,    13,     2,   206],\n",
            "        [ 3733,     6,   115,     3, 14846,    18,     9,    46,    20,   139,\n",
            "            10,    24,     6,  1279,   238],\n",
            "        [   28,     4,   108,  3006,   123,     9,  6990,   140,  1713,  2782,\n",
            "             5,  1833,   973,   190,   474],\n",
            "        [   38,  1812,    34,   996,     3,   882,  1144,     4,  3403,  4479,\n",
            "            30,   499, 22512,  2148,  1462],\n",
            "        [    7,   219,   337,    52,    79,   512,   802,     4,     2,   237,\n",
            "         22770,  1685,  2168,   125,  3311]], device='cuda:0')\n",
            "tweet after swap tensor([[   38,  1812,  3197,     9,    38,    90,  2694,     6,   113,     2,\n",
            "           203, 10311,     4,     2,    60],\n",
            "        [   29,     3,  3773,     4, 13172,  5976,    33,     2, 20829,  1025,\n",
            "          5699,     2,  3827,   750,  7607],\n",
            "        [    2, 23562,  1314,    56, 16075,    61,   108,   358,    53,    32,\n",
            "            64,  2563,    13, 13327,  1554],\n",
            "        [ 2081,  1350,     7, 24801,    10,  4530,  4582,   598,    23,   241,\n",
            "            11,    40,    10,   172,    20],\n",
            "        [   10,   158,   881,    46,    20,   245,     2, 14250,  1678,     5,\n",
            "             2,  7726, 23250,   211,    36],\n",
            "        [   39,  1828,    50,  5438,     3,    47,   410,    70,     6,   330,\n",
            "             2,   183, 22673,    12,  1387],\n",
            "        [   10,    73,  7698,   938, 10329,    15,  2802,    24,     3,   451,\n",
            "             9,    55,    34,    21,  3479],\n",
            "        [    2,  1400,  4025,   242,  3539,  1920,     6, 22532,   124,     2,\n",
            "          9071,     4,     9,  5943,  2922],\n",
            "        [    2,   972,  8126,    12,   536,  1880,     7, 25094,    34,     3,\n",
            "           449,     8,    14,   109,   207],\n",
            "        [   37,    10,   248,   634,     6,  1728,     5,  4352,    22,    11,\n",
            "            14,  1205,   753,  1993,  1309],\n",
            "        [ 3733,     6,   115,     3, 14846,    18,     9,    46,    20,   139,\n",
            "            10,    24,     6,  1279,   238],\n",
            "        [ 5131,   923,     7,    48,   169,    30,   724,   362, 10558,     4,\n",
            "          3035,     5, 13005,     2,  9766],\n",
            "        [   37,    10,   248,   634,     6,  1728,     5,  4352,    22,    11,\n",
            "            14,  1205,   753,  1993,  1309],\n",
            "        [   36,     9,    39,  3106,  1073,    50,   312,     7,  1553,     8,\n",
            "             2,   786, 16765,    44,   665],\n",
            "        [  188,  2482, 12244,  5572,     2, 15193,     4,     3,   352,   939,\n",
            "         20127,    44,     7,     3,  4281],\n",
            "        [   45,    14, 23576,  2029,    54,  5305,     8, 19394,     3,   161,\n",
            "            29,    33,     2,   620,  1331],\n",
            "        [ 5131,   923,     7,    48,   169,    30,   724,   362, 10558,     4,\n",
            "          3035,     5, 13005,     2,  9766],\n",
            "        [ 5131,   923,     7,    48,   169,    30,   724,   362, 10558,     4,\n",
            "          3035,     5, 13005,     2,  9766],\n",
            "        [    3,  2934,  1189,    13, 17292,     3, 15675,  7551,     9,    46,\n",
            "            20,   485,  4596,     2,   141],\n",
            "        [   40,    19,    55,   113,   417,     2,  7986,   246, 18068,    56,\n",
            "            21,   104,   202,    69,    62],\n",
            "        [    3, 18389,   256,     4, 14094,     9,  1046,     6,  1721,   194,\n",
            "           124,    11, 15644,  1212,   259],\n",
            "        [  178,  3477,     8,     3,    58, 25774,    10,     7,    16,   971,\n",
            "            17,    13,     2,    57,   322],\n",
            "        [    6, 19556,    56,  3134,   103,     9,  1685,  2168,   125,   283,\n",
            "             7,   132,  1044,     5,  4826],\n",
            "        [   31,     3,  2257,  2880,  4067,   953,   941,    44, 17625,    49,\n",
            "          7984,     5,    34,    70,  3599],\n",
            "        [   37,    10,   248,   634,     6,  1728,     5,  4352,    22,    11,\n",
            "            14,  1205,   753,  1993,  1309],\n",
            "        [    2,   290,   226,  5082,     2,   462,    27,     2,  2429,     4,\n",
            "           165,   695,   200,   992,   676],\n",
            "        [   10,    96,   230,   224,     6,   113,  7075,     5,   204,   115,\n",
            "            15,    18,    12,   177,  2645],\n",
            "        [  137,  3346, 25895,   257,   233,   402, 22591,  5499,    74,    32,\n",
            "            29,  7627,    13,     2,   206],\n",
            "        [ 3733,     6,   115,     3, 14846,    18,     9,    46,    20,   139,\n",
            "            10,    24,     6,  1279,   238],\n",
            "        [   28,     4,   108,  3006,   123,     9,  6990,   140,  1713,  2782,\n",
            "             5,  1833,   973,   190,   474],\n",
            "        [   38,  1812,    34,   996,     3,   882,  1144,     4,  3403,  4479,\n",
            "            30,   499, 22512,  2148,  1462],\n",
            "        [    7,   219,   337,    52,    79,   512,   802,     4,     2,   237,\n",
            "         22770,  1685,  2168,   125,  3311]], device='cuda:0')\n",
            "tweet after delete [tensor([    2, 23562,  1314,    56, 16075,    61,   108,   358,    53,    32,\n",
            "           64,  2563,    13, 13327,  1554], device='cuda:0'), tensor([ 2081,  1350,     7, 24801,    10,  4530,  4582,   598,    23,   241,\n",
            "           11,    40,    10,   172,    20], device='cuda:0'), tensor([   10,    73,  7698,   938, 10329,    15,  2802,    24,     3,   451,\n",
            "            9,    55,    34,    21,  3479], device='cuda:0'), tensor([    2,  1400,  4025,   242,  3539,  1920,     6, 22532,   124,     2,\n",
            "         9071,     4,     9,  5943,  2922], device='cuda:0'), tensor([  37,   10,  248,  634,    6, 1728,    5, 4352,   22,   11,   14, 1205,\n",
            "         753, 1993, 1309], device='cuda:0'), tensor([   36,     9,    39,  3106,  1073,    50,   312,     7,  1553,     8,\n",
            "            2,   786, 16765,    44,   665], device='cuda:0'), tensor([ 5131,   923,     7,    48,   169,    30,   724,   362, 10558,     4,\n",
            "         3035,     5, 13005,     2,  9766], device='cuda:0'), tensor([  178,  3477,     8,     3,    58, 25774,    10,     7,    16,   971,\n",
            "           17,    13,     2,    57,   322], device='cuda:0'), tensor([   31,     3,  2257,  2880,  4067,   953,   941,    44, 17625,    49,\n",
            "         7984,     5,    34,    70,  3599], device='cuda:0'), tensor([   2,  290,  226, 5082,    2,  462,   27,    2, 2429,    4,  165,  695,\n",
            "         200,  992,  676], device='cuda:0'), tensor([  137,  3346, 25895,   257,   233,   402, 22591,  5499,    74,    32,\n",
            "           29,  7627,    13,     2,   206], device='cuda:0'), tensor([  28,    4,  108, 3006,  123,    9, 6990,  140, 1713, 2782,    5, 1833,\n",
            "         973,  190,  474], device='cuda:0'), tensor([   38,  1812,    34,   996,     3,   882,  1144,     4,  3403,  4479,\n",
            "           30,   499, 22512,  2148,  1462], device='cuda:0')]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-b38cce5877b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# evaluate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-76-db1345d47818>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, iterator, optimizer, criterion)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# convert to 1D tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtweet_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0;31m# compute the loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-28-d3b34713e307>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, text_lengths)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# text = [batch size, sent_length]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0;31m# embedded = [batch size, sent_len, emb dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    156\u001b[0m         return F.embedding(\n\u001b[1;32m    157\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   1914\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1915\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1916\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1917\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: embedding(): argument 'indices' (position 2) must be Tensor, not list"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zH72ILM8WICB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}